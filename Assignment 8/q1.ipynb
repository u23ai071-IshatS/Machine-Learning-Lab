{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34466922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b922b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"drug_200.csv\")\n",
    "\n",
    "df['Sex'] = df['Sex'].map({'F': 0, 'M': 1})\n",
    "df['Cholesterol'] = df['Cholesterol'].map({'NORMAL': 0, 'HIGH': 1})\n",
    "\n",
    "bp_ohe = OneHotEncoder(sparse_output=False, drop=None)\n",
    "bp_encoded = bp_ohe.fit_transform(df[['BP']])\n",
    "bp_feature_names = bp_ohe.get_feature_names_out(['BP'])\n",
    "bp_df = pd.DataFrame(bp_encoded, columns=bp_feature_names, index=df.index)\n",
    "\n",
    "num_df = df[['Age', 'Na_to_K']].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_scaled = scaler.fit_transform(num_df)\n",
    "\n",
    "X = np.hstack([num_scaled, df[['Sex', 'Cholesterol']].values, bp_df.values])\n",
    "\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Drug'].values)\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "Y_onehot = np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20065127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    expz = np.exp(z)\n",
    "    return expz / np.sum(expz, axis=1, keepdims=True)\n",
    "\n",
    "def compute_loss(X, Y_onehot, W, reg_type=None, lam=0.0, alpha=0.5):\n",
    "    N = X.shape[0]\n",
    "    logits = X @ W\n",
    "    probs = softmax(logits)\n",
    "    # negative log-likelihood / cross-entropy\n",
    "    loss = -np.sum(Y_onehot * np.log(probs + 1e-15)) / N\n",
    "\n",
    "    # exclude bias (first row of W corresponds to bias weights for each class)\n",
    "    W_no_bias = W.copy()\n",
    "    W_no_bias[0, :] = 0.0\n",
    "\n",
    "    if reg_type == \"l1\":\n",
    "        loss += lam * np.sum(np.abs(W_no_bias))\n",
    "    elif reg_type == \"l2\":\n",
    "        loss += lam * np.sum(W_no_bias ** 2)\n",
    "    elif reg_type == \"elasticnet\":\n",
    "        loss += lam * (alpha * np.sum(np.abs(W_no_bias)) + (1 - alpha) * np.sum(W_no_bias ** 2))\n",
    "    return loss\n",
    "\n",
    "def compute_gradient(X, Y_onehot, W, reg_type=None, lam=0.0, alpha=0.5):\n",
    "    N = X.shape[0]\n",
    "    logits = X @ W\n",
    "    probs = softmax(logits)\n",
    "    grad = (X.T @ (probs - Y_onehot)) / N  # shape (n_features, n_classes)\n",
    "\n",
    "    # Regularize but exclude bias row (row 0)\n",
    "    if reg_type is not None and lam > 0:\n",
    "        reg_term = np.zeros_like(W)\n",
    "        if reg_type == \"l1\":\n",
    "            reg_term = lam * np.sign(W)\n",
    "        elif reg_type == \"l2\":\n",
    "            reg_term = 2 * lam * W\n",
    "        elif reg_type == \"elasticnet\":\n",
    "            reg_term = lam * (alpha * np.sign(W) + 2 * (1 - alpha) * W)\n",
    "\n",
    "        reg_term[0, :] = 0.0  # exclude bias from regularization\n",
    "        grad += reg_term\n",
    "\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314dac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_softmax(X_train, Y_train_onehot, lr=0.05, epochs=1500, reg_type=None, lam=0.0, alpha=0.5, verbose=False):\n",
    "    n_features = X_train.shape[1]\n",
    "    n_classes = Y_train_onehot.shape[1]\n",
    "    W = np.zeros((n_features, n_classes))\n",
    "    for epoch in range(epochs):\n",
    "        grad = compute_gradient(X_train, Y_train_onehot, W, reg_type, lam, alpha)\n",
    "        W -= lr * grad\n",
    "        if verbose and (epoch % 300 == 0 or epoch == epochs-1):\n",
    "            loss = compute_loss(X_train, Y_train_onehot, W, reg_type, lam, alpha)\n",
    "            print(f\" epoch {epoch:4d}  loss={loss:.6f}\")\n",
    "    return W\n",
    "\n",
    "def predict(X, W):\n",
    "    probs = softmax(X @ W)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f4f601d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, Y_onehot, reg_type=None, lam=0.0, alpha=0.5, lr=0.05, epochs=1500):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    metrics = {\"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_tr, X_te = X[train_idx], X[test_idx]\n",
    "        Ytr_onehot, y_te = Y_onehot[train_idx], y[test_idx]\n",
    "\n",
    "        W = train_softmax(X_tr, Ytr_onehot, lr=lr, epochs=epochs,\n",
    "                          reg_type=reg_type, lam=lam, alpha=alpha, verbose=False)\n",
    "        y_pred = predict(X_te, W)\n",
    "\n",
    "        metrics[\"accuracy\"].append(accuracy_score(y_te, y_pred))\n",
    "        metrics[\"precision\"].append(precision_score(y_te, y_pred, average='macro', zero_division=0))\n",
    "        metrics[\"recall\"].append(recall_score(y_te, y_pred, average='macro', zero_division=0))\n",
    "        metrics[\"f1\"].append(f1_score(y_te, y_pred, average='macro', zero_division=0))\n",
    "    return {k: np.mean(v) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddba8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training & evaluating...\n",
      "\n",
      "5-Fold CV results (macro averaged):\n",
      "        Model  Accuracy  Precision  Recall  F1-score\n",
      "0      No Reg     0.935     0.9004  0.8774    0.8792\n",
      "1  Lasso (L1)     0.950     0.9153  0.9085    0.9075\n",
      "2  Ridge (L2)     0.935     0.9004  0.8774    0.8792\n",
      "3  ElasticNet     0.940     0.9021  0.8819    0.8824\n"
     ]
    }
   ],
   "source": [
    "print(\"Training & evaluating...\")\n",
    "\n",
    "base = cross_validate(X, y, Y_onehot, reg_type=None, lam=0.0)\n",
    "lasso = cross_validate(X, y, Y_onehot, reg_type=\"l1\", lam=0.005)\n",
    "ridge = cross_validate(X, y, Y_onehot, reg_type=\"l2\", lam=0.005)\n",
    "elastic = cross_validate(X, y, Y_onehot, reg_type=\"elasticnet\", lam=0.005, alpha=0.5)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"No Reg\", \"Lasso (L1)\", \"Ridge (L2)\", \"ElasticNet\"],\n",
    "    \"Accuracy\": [base[\"accuracy\"], lasso[\"accuracy\"], ridge[\"accuracy\"], elastic[\"accuracy\"]],\n",
    "    \"Precision\": [base[\"precision\"], lasso[\"precision\"], ridge[\"precision\"], elastic[\"precision\"]],\n",
    "    \"Recall\": [base[\"recall\"], lasso[\"recall\"], ridge[\"recall\"], elastic[\"recall\"]],\n",
    "    \"F1-score\": [base[\"f1\"], lasso[\"f1\"], ridge[\"f1\"], elastic[\"f1\"]],\n",
    "})\n",
    "\n",
    "print(\"\\n5-Fold CV results (macro averaged):\")\n",
    "print(results_df.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
